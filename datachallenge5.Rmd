---
title: "datachallenge5"
author: "menglu zhao"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(purrr)
library(tidyverse)
library(dendextend)
```

## Problem 1

### The appropriate number of clusters for this data is one, when k = 2. it appears that there is an “elbow” at k = 2 clusters, where the total within sum of squares begins to level off.

Cluster 1: All 20 samples in this cluster are labeled as 'Diseased'.
Cluster 2: All 20 samples in this cluster are labeled as 'Healthy'.

```{r}
data <- read.csv("/Users/lulu/Downloads/Ch10Ex11.csv",header=FALSE)
data <- t(data)  # Transpose the data
data <- scale(data)  # Scale the data

set.seed(100) # introduces randomness
wss <- function(k, data) {
  kmeans(data, k, nstart = 50)$tot.withinss
} # function to find within sum of square

k_values <- 1:15
wss_values <- map_dbl(k_values, wss, data = data)  

wss_values <- tibble(wss = wss_values,
                     k = k_values)
ggplot(wss_values,
            aes(x = k,
                y = wss)) +
    geom_point() +
geom_line() # create plot of within-cluster sum of squares vs. cluster size

set.seed(123)
k <- 2  # number of clusters from the elbow plot
km<- kmeans(data, centers = k, nstart = 50)

disease_status <- c(rep("Healthy", 20), rep("Diseased", 20))
table(Cluster = km$cluster, DiseaseStatus = disease_status) # cluster assignment of each observation

```

## Problem 2 Perform hierarchical clustering on the same scaled data set.

Complete Linkage (Euclidean Distance)
Cluster 1: 20 samples, Cluster 2: 9 samples, Cluster 3: 11 samples

Single Linkage (Euclidean Distance)
Cluster 1: 19 samples, Cluster 2: 1 sample, Cluster 3: 20 samples

Average Linkage (Euclidean Distance)
Cluster 1: 20 samples, Cluster 2: 19 samples, Cluster 3: 1 sample

```{r, out.width = "100%", fig.dim=c(20,10)}
dist_data <- dist(data)  #  calculate and inter-observation 'Eucledian distance' matrix
complete_euclidean <- hclust(dist_data, method = "complete")
single_euclidean <- hclust(dist_data, method = "single")
average_euclidean <- hclust(dist_data, method = "average")

plot(complete_euclidean, main = "Complete Linkage (Euclidean)", cex = 0.5)
plot(single_euclidean, main = "Single Linkage (Euclidean)", cex = 0.5)
plot(average_euclidean, main = "Average Linkage (Euclidean)", cex = 0.5)

# cutree() function is used to cut a hierarchical clustering tree into a specified number of clusters
clusters_complete_euclidean <- cutree(complete_euclidean, k = 3) # k parameter is set to 3, which means that the tree will be cut into 3 clusters.
clusters_single_euclidean <- cutree(single_euclidean, k = 3)
clusters_average_euclidean <- cutree(average_euclidean, k = 3)

# Create contingency tables
table(clusters_complete_euclidean)
table(clusters_single_euclidean)
table(clusters_average_euclidean)
```
## Problem 3 Write a few sentences commenting on the results you obtained in Problems 1 and 2.

