---
title: "datachallenge5"
author: "menglu zhao"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(purrr)
library(tidyverse)
library(dendextend)
```

## link: https://github.com/mezlulu/datachallenge5

## Problem 1

The appropriate number of clusters for this data is one, when k = 2. it appears that there is an “elbow” at k = 2 clusters, where the total within sum of squares begins to level off.

Cluster 1: All 20 samples in this cluster are labeled as 'Diseased'. Cluster 2: All 20 samples in this cluster are labeled as 'Healthy'. By the elbow plot, each cluster is perfectly separated into either 'Diseased' or 'Healthy'. Cluster 1 contains only healthy individuals, and Cluster 2 contains only diseased individuals. There appears to be a strong association between the cluster assignment and disease status since the counts are exclusively in off-diagonal cells. 

```{r}
data <- read.csv("/Users/lulu/Downloads/Ch10Ex11.csv",header=FALSE)
data <- t(data)  # Transpose the data
data <- scale(data)  # Scale the data

set.seed(100) # introduces randomness
wss <- function(k, data) {
  kmeans(data, k, nstart = 50)$tot.withinss
} # function to find within sum of square

k_values <- 1:15
wss_values <- map_dbl(k_values, wss, data = data)  

wss_values <- tibble(wss = wss_values,
                     k = k_values)
ggplot(wss_values,
            aes(x = k,
                y = wss)) +
    geom_point() +
geom_line() # create plot of within-cluster sum of squares vs. cluster size

set.seed(100)
k <- 2  # number of clusters from the elbow plot
km<- kmeans(data, centers = k, nstart = 50)

disease_status <- c(rep("Healthy", 20), rep("Diseased", 20))
table(Cluster = km$cluster, DiseaseStatus = disease_status) # cluster assignment of each observation

```

## Problem 2 Perform hierarchical clustering on the same scaled data set.

Complete Linkage
Cluster 1: 20 samples, Cluster 2: 9 samples, Cluster 3: 11 samples
Complete linkage uses the maximum distance between points in two clusters to form a new cluster. Create a relatively balanced distribution across the clusters, without any single cluster dominating.

Single Linkage 
Cluster 1: 19 samples, Cluster 2: 1 sample, Cluster 3: 20 samples
Single linkage uses the minimum distance between points in two clusters for clustering. Clusters can be strung together, leading to one large cluster and several smaller ones, in this case here with one cluster having only 1 item.

Average Linkage 
Cluster 1: 20 samples, Cluster 2: 19 samples, Cluster 3: 1 sample
Average linkage takes the average distance between all pairs of points in two clusters. This method has produced two large clusters and one very small cluster, suggesting that most data points are more similar to each other than to the points in the small cluster.

```{r, out.width = "100%", fig.dim=c(20,10)}
dist_data <- dist(data)  #  calculate and inter-observation 'Eucledian distance' matrix
complete_euclidean <- hclust(dist_data, method = "complete")
single_euclidean <- hclust(dist_data, method = "single")
average_euclidean <- hclust(dist_data, method = "average")

plot(complete_euclidean, main = "Complete Linkage (Euclidean)", cex = 0.5)
plot(single_euclidean, main = "Single Linkage (Euclidean)", cex = 0.5)
plot(average_euclidean, main = "Average Linkage (Euclidean)", cex = 0.5)

# cutree() function is used to cut a hierarchical clustering tree into a specified number of clusters
clusters_complete_euclidean <- cutree(complete_euclidean, k = 3) # k parameter is set to 3, which means that the tree will be cut into 3 clusters.
clusters_single_euclidean <- cutree(single_euclidean, k = 3)
clusters_average_euclidean <- cutree(average_euclidean, k = 3)

# Create contingency tables
table(clusters_complete_euclidean)
table(clusters_single_euclidean)
table(clusters_average_euclidean)
```
## Problem 3 Write a few sentences commenting on the results you obtained in Problems 1 and 2.

The first contingency table shows a perfect separation between the clusters and disease status, with one cluster containing all the diseased cases and the other containing all the healthy cases. The Hierarchical Clustering table vary depending on what linkage method is used. Different distribution of cases across clusters for complete, single, and average linkage methods.  Complete linkage results in the most balanced cluster sizes, which might be suitable for applications where a more uniform cluster size is desired. Single and average linkage have resulted in one cluster with a single item, indicating the presence of outliers or very dissimilar observations that do not group well with others.

The k-means clustering method appears to be more effective for creating clusters in this case, possibly due to its centroid-based approach that optimizes for variance within clusters. Hierarchical clustering does not align clusters as well with the disease status.